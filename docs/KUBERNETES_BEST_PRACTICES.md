# üéØ KUBERNETES BEST PRACTICES

**Projeto:** The Throne of Games  
**Data:** 07/01/2026  
**Vers√£o:** 1.0

---

## üìã √çNDICE

1. [Resource Management](#resource-management)
2. [High Availability](#high-availability)
3. [Security](#security)
4. [Monitoring & Observability](#monitoring--observability)
5. [CI/CD & GitOps](#cicd--gitops)
6. [Cost Optimization](#cost-optimization)
7. [Disaster Recovery](#disaster-recovery)
8. [Performance Tuning](#performance-tuning)

---

## üíæ RESOURCE MANAGEMENT

### Sempre Definir Requests e Limits

‚úÖ **CORRETO:**
```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "300m"
  limits:
    memory: "2Gi"
    cpu: "1500m"
```

‚ùå **EVITAR:**
```yaml
# Sem defini√ß√£o de recursos
# Pode causar OOMKilled ou throttling inesperado
```

### Requests vs Limits

- **Requests**: Recursos garantidos pelo scheduler
- **Limits**: M√°ximo que o pod pode usar

**Regra de ouro:**
```
requests = uso m√©dio esperado
limits = picos m√°ximos permitidos
```

### QoS Classes

```yaml
# Guaranteed (melhor QoS)
requests.memory == limits.memory
requests.cpu == limits.cpu

# Burstable (QoS m√©dio)
requests.memory < limits.memory

# BestEffort (pior QoS, ser√° morto primeiro)
# Sem requests nem limits
```

### Resource Quotas por Namespace

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: thethroneofgames
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "5"
    pods: "50"
```

---

## üèóÔ∏è HIGH AVAILABILITY

### Multi-Replica Deployments

```yaml
# M√çNIMO para produ√ß√£o
replicas: 3

# Distribuir em m√∫ltiplas zonas
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - usuarios-api
        topologyKey: topology.kubernetes.io/zone
```

### Pod Disruption Budgets

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: usuarios-api-pdb
  namespace: thethroneofgames
spec:
  minAvailable: 2  # Sempre manter 2 pods dispon√≠veis
  selector:
    matchLabels:
      app: usuarios-api
      tier: backend
```

### Health Probes Corretas

```yaml
# Liveness: Detecta deadlocks (kill e restart)
livenessProbe:
  httpGet:
    path: /health/live
    port: 5001
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# Readiness: Detecta se pode receber tr√°fego
readinessProbe:
  httpGet:
    path: /health/ready
    port: 5001
  initialDelaySeconds: 15
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 3
```

**Importante:**
- Liveness ‚â† Readiness
- Liveness muito agressivo = restart loops
- Readiness: pode ser temporariamente unhealthy

### Graceful Shutdown

```yaml
# No Deployment
terminationGracePeriodSeconds: 30

# No c√≥digo .NET
var lifetime = app.Services.GetRequiredService<IHostApplicationLifetime>();
lifetime.ApplicationStopping.Register(() =>
{
    logger.LogInformation("Graceful shutdown iniciado");
    // Finalizar conex√µes, processar fila, etc.
});
```

---

## üîí SECURITY

### Princ√≠pios

1. **Least Privilege**: M√≠nimos privil√©gios necess√°rios
2. **Defense in Depth**: M√∫ltiplas camadas de seguran√ßa
3. **Zero Trust**: Nenhuma comunica√ß√£o √© confi√°vel por padr√£o

### Network Policies (Zero Trust)

```yaml
# 1. Negar tudo por padr√£o
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

# 2. Permitir apenas o necess√°rio
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-to-db
spec:
  podSelector:
    matchLabels:
      app: sqlserver
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 1433
```

### Pod Security Standards

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: thethroneofgames
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

### Security Context

```yaml
securityContext:
  # Pod-level
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 2000
  seccompProfile:
    type: RuntimeDefault
  
  # Container-level
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
```

### Secrets Management

**Op√ß√µes (da mais simples para mais robusta):**

1. **Kubernetes Secrets (nativo)**
   ```powershell
   kubectl create secret generic app-secrets \
     --from-literal=jwt-secret=your-secret \
     --from-literal=db-password=your-password
   ```

2. **Sealed Secrets** (GitOps-friendly)
   ```powershell
   kubeseal --format yaml < secret.yaml > sealed-secret.yaml
   # Pode commitar sealed-secret.yaml no Git
   ```

3. **External Secrets Operator** (recomendado)
   - Integra com Azure Key Vault, AWS Secrets Manager, GCP Secret Manager
   - Secrets nunca ficam no Git
   ```yaml
   apiVersion: external-secrets.io/v1beta1
   kind: ExternalSecret
   metadata:
     name: app-secrets
   spec:
     secretStoreRef:
       name: azure-keyvault
       kind: SecretStore
     target:
       name: app-secrets
     data:
     - secretKey: jwt-secret
       remoteRef:
         key: jwt-secret
   ```

### RBAC (Role-Based Access Control)

```yaml
# ServiceAccount para a aplica√ß√£o
apiVersion: v1
kind: ServiceAccount
metadata:
  name: usuarios-api-sa
  namespace: thethroneofgames

---
# Role com permiss√µes m√≠nimas
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: usuarios-api-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]

---
# RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: usuarios-api-binding
subjects:
- kind: ServiceAccount
  name: usuarios-api-sa
roleRef:
  kind: Role
  name: usuarios-api-role
  apiGroup: rbac.authorization.k8s.io
```

---

## üìä MONITORING & OBSERVABILITY

### 4 Pilares da Observabilidade

1. **Metrics** (Prometheus)
2. **Logs** (EFK/Loki)
3. **Traces** (Jaeger/Tempo)
4. **Events** (Kubernetes Events)

### Golden Signals

Monitore estas 4 m√©tricas:

1. **Latency**: Tempo de resposta
   ```promql
   histogram_quantile(0.95, 
     rate(http_request_duration_seconds_bucket[5m])
   )
   ```

2. **Traffic**: Requisi√ß√µes por segundo
   ```promql
   rate(http_requests_total[5m])
   ```

3. **Errors**: Taxa de erro
   ```promql
   rate(http_requests_total{status=~"5.."}[5m])
   / rate(http_requests_total[5m])
   ```

4. **Saturation**: Uso de recursos
   ```promql
   container_memory_working_set_bytes
   / container_spec_memory_limit_bytes
   ```

### Alertas Essenciais

```yaml
# Prometheus AlertManager
groups:
- name: thethroneofgames
  rules:
  # Pod crashlooping
  - alert: PodCrashLooping
    expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Pod {{ $labels.pod }} est√° crashlooping"

  # Alta lat√™ncia
  - alert: HighLatency
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "P95 latency acima de 1s"

  # Alta taxa de erro
  - alert: HighErrorRate
    expr: |
      rate(http_requests_total{status=~"5.."}[5m])
      / rate(http_requests_total[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Taxa de erro acima de 5%"

  # Pods n√£o prontos
  - alert: PodsNotReady
    expr: kube_pod_status_ready{condition="false"} > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Pod {{ $labels.pod }} n√£o est√° Ready"
```

### Structured Logging

```csharp
// .NET Serilog configuration
Log.Logger = new LoggerConfiguration()
    .Enrich.FromLogContext()
    .Enrich.WithMachineName()
    .Enrich.WithProperty("Application", "UsuariosAPI")
    .Enrich.WithProperty("Environment", Environment.GetEnvironmentVariable("ASPNETCORE_ENVIRONMENT"))
    .WriteTo.Console(new JsonFormatter())
    .CreateLogger();

// Log com contexto
logger.LogInformation(
    "Pedido criado: {PedidoId} - Usuario: {UsuarioId} - Valor: {Valor}",
    pedido.Id, pedido.UsuarioId, pedido.ValorTotal
);
```

---

## üîÑ CI/CD & GITOPS

### GitOps Principles

1. **Declarative**: Tudo em YAML/Helm charts
2. **Versioned**: Git como single source of truth
3. **Automated**: Deploys autom√°ticos via Pull
4. **Auditable**: Hist√≥rico completo no Git

### ArgoCD Setup (Recomendado)

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: thethroneofgames
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/guilhermesoatto/TheThroneOfGames.git
    targetRevision: HEAD
    path: k8s
  destination:
    server: https://kubernetes.default.svc
    namespace: thethroneofgames
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
    - CreateNamespace=true
```

### Deployment Strategies

#### 1. Rolling Update (Padr√£o)
```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 1
```
‚úÖ Simples, zero downtime  
‚ùå Vers√µes misturadas temporariamente

#### 2. Blue-Green (Implementado no CI/CD)
```yaml
# Blue (atual)
app: usuarios-api
version: v1.0

# Green (nova)
app: usuarios-api
version: v1.1

# Switch de tr√°fego
kubectl patch service usuarios-api -p '{"spec":{"selector":{"version":"v1.1"}}}'
```
‚úÖ Rollback instant√¢neo, sem mistura de vers√µes  
‚ùå Mais recursos (2x pods temporariamente)

#### 3. Canary (Progressivo)
```yaml
# Implementar com Istio/Linkerd
# 10% ‚Üí 25% ‚Üí 50% ‚Üí 100%
```
‚úÖ Menor risco, testa com usu√°rios reais  
‚ùå Complexo, precisa service mesh

### Image Tagging Strategy

```yaml
# ‚úÖ CORRETO: Multi-tag
ghcr.io/org/app:v1.2.3        # Semver
ghcr.io/org/app:abc123d       # Git SHA
ghcr.io/org/app:master        # Branch
ghcr.io/org/app:latest        # Latest (n√£o usar em produ√ß√£o!)

# ‚ùå EVITAR: Apenas latest
ghcr.io/org/app:latest
```

---

## üí∞ COST OPTIMIZATION

### Vertical Pod Autoscaler (VPA)

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: usuarios-api-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: usuarios-api
  updatePolicy:
    updateMode: "Recreate"  # Ou "Initial", "Off"
```

Analisa uso real e ajusta requests/limits automaticamente.

### Cluster Autoscaler

```yaml
# Em cloud providers
# Adiciona/remove nodes baseado na demanda
```

### Karpenter (AWS) ou KEDA (Event-driven)

```yaml
# KEDA: Escala baseado em eventos
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: rabbitmq-scaler
spec:
  scaleTargetRef:
    name: pedidos-processor
  triggers:
  - type: rabbitmq
    metadata:
      queueName: pedidos
      queueLength: "10"
```

### Spot Instances / Preemptible VMs

```yaml
# Tolerations para nodes spot
tolerations:
- key: "kubernetes.azure.com/scalesetpriority"
  operator: "Equal"
  value: "spot"
  effect: "NoSchedule"

# Ou node affinity
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1
      preference:
        matchExpressions:
        - key: karpenter.sh/capacity-type
          operator: In
          values:
          - spot
```

**Economia:** 60-80% vs On-Demand  
**Trade-off:** Pode ser interrompido a qualquer momento

---

## üî• DISASTER RECOVERY

### Backup Strategy (3-2-1 Rule)

- **3** c√≥pias dos dados
- **2** tipos de m√≠dia diferentes
- **1** c√≥pia offsite

### Velero (Kubernetes Backup)

```powershell
# Instalar Velero
velero install --provider azure --use-volume-snapshots=true

# Backup do namespace
velero backup create thethroneofgames-backup \
  --include-namespaces thethroneofgames

# Schedule autom√°tico
velero schedule create daily-backup \
  --schedule="0 2 * * *" \
  --include-namespaces thethroneofgames

# Restore
velero restore create --from-backup thethroneofgames-backup
```

### Database Backup

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sqlserver-backup
spec:
  schedule: "0 2 * * *"  # 2 AM daily
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: mcr.microsoft.com/mssql-tools
            command:
            - /bin/bash
            - -c
            - |
              /opt/mssql-tools/bin/sqlcmd -S sqlserver-service \
                -U sa -P $SA_PASSWORD \
                -Q "BACKUP DATABASE [PlataformaJogos] TO DISK = '/backup/db-$(date +%Y%m%d).bak'"
            volumeMounts:
            - name: backup
              mountPath: /backup
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
```

### RTO & RPO

**RTO (Recovery Time Objective):** Tempo m√°ximo de indisponibilidade  
**RPO (Recovery Point Objective):** M√°ximo de dados que pode ser perdido

```
Nosso target:
- RTO: < 1 hora
- RPO: < 15 minutos

Estrat√©gias:
- Backups a cada 15 min (RPO)
- Hot standby em outra regi√£o (RTO)
- Automated disaster recovery playbook
```

---

## ‚ö° PERFORMANCE TUNING

### Database Connection Pooling

```csharp
// appsettings.json
"ConnectionStrings": {
  "DefaultConnection": "Server=sqlserver-service;Database=PlataformaJogos;User Id=sa;Password=...;Min Pool Size=10;Max Pool Size=100;Connection Lifetime=300;"
}
```

### Redis Caching

```yaml
# Deployment do Redis
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 1
  template:
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "1Gi"
            cpu: "500m"
```

```csharp
// .NET Configuration
services.AddStackExchangeRedisCache(options =>
{
    options.Configuration = Configuration["Redis:ConnectionString"];
    options.InstanceName = "PlataformaJogos:";
});

// Uso
await cache.SetStringAsync($"jogo:{id}", JsonSerializer.Serialize(jogo), 
    new DistributedCacheEntryOptions {
        AbsoluteExpirationRelativeToNow = TimeSpan.FromMinutes(30)
    });
```

### HTTP Keep-Alive & Connection Reuse

```csharp
// HttpClient factory (singleton)
services.AddHttpClient("ExternalAPI", client =>
{
    client.BaseAddress = new Uri("https://api.externa.com");
    client.Timeout = TimeSpan.FromSeconds(30);
})
.ConfigurePrimaryHttpMessageHandler(() => new HttpClientHandler
{
    MaxConnectionsPerServer = 20,
    UseProxy = false
});
```

### Async/Await Best Practices

```csharp
// ‚úÖ CORRETO
public async Task<Pedido> CriarPedidoAsync(PedidoDto dto)
{
    var pedido = new Pedido();
    await _repository.AddAsync(pedido);
    await _eventBus.PublishAsync(new PedidoCriadoEvent(pedido.Id));
    return pedido;
}

// ‚ùå EVITAR: Bloqueio com .Result ou .Wait()
public Pedido CriarPedido(PedidoDto dto)
{
    var pedido = new Pedido();
    _repository.AddAsync(pedido).Result;  // Deadlock risk!
    return pedido;
}
```

---

## üìö CHECKLIST DE PRODU√á√ÉO

### Antes do Go-Live

#### Infrastructure
- [ ] Multi-node cluster (m√≠nimo 3 nodes)
- [ ] Auto-scaling configurado (HPA + CA)
- [ ] Persistent volumes com backups
- [ ] Network policies ativas
- [ ] Ingress com TLS v√°lido
- [ ] DNS configurado

#### Application
- [ ] Replicas >= 3 por deployment
- [ ] Health probes funcionando
- [ ] Resources requests/limits definidos
- [ ] Graceful shutdown implementado
- [ ] Connection pooling otimizado
- [ ] Caching implementado

#### Security
- [ ] Secrets em vault (n√£o em Git)
- [ ] RBAC configurado
- [ ] Pod Security Standards
- [ ] Vulnerability scanning no CI/CD
- [ ] Security Context definido
- [ ] Network policies testadas

#### Monitoring
- [ ] Prometheus + Grafana instalado
- [ ] Dashboards criados
- [ ] Alertas configurados
- [ ] Logs centralizados
- [ ] Tracing distribu√≠do
- [ ] Runbooks documentados

#### CI/CD
- [ ] Pipeline completo (build, test, deploy)
- [ ] Blue-Green ou Canary deployment
- [ ] Automated rollback
- [ ] Performance tests no pipeline
- [ ] Ambientes: dev, staging, prod

#### Disaster Recovery
- [ ] Backups automatizados
- [ ] Restore testado
- [ ] RTO/RPO documentados
- [ ] DR playbook criado
- [ ] Multi-region setup (se aplic√°vel)

---

**√öltima atualiza√ß√£o:** 07/01/2026  
**Autor:** DevOps Team  
**Vers√£o:** 1.0
